= Проект выпускника курсов OTUS Data engineering Клюсы А.Ю.

== Тема

Создание архитектуры по сбору, хранению и отслеживанию данных о погоде
из открытого источника http://openweathermap.org

== Постановка задачи

Я решил создать проект - основу для построения хранения и анализа
общедоступной информации.

Цель проекта - построить максимально недорогую открытую систему
для приема, хранения и анализа любой общедоступной информации
из открытых (а, возможно, и закрытых) источников.

Компоненты и решения, ввиду ограниченного срока разработки,
решено использовать классические и известные мне с курсов.
Более того, я использовал их docker-версии.

Система должна основываться на свободных, желательно open-source,
в крайнем случае недорогих, но эффективных и распространенных решениях.

Система должна быть способна к расширению
(добавление новых источников и приемников в хранилище под различные кейсы)
и масштабированию (компоненты системы могут объединяться в кластера)

При создании системы я старался придерживаться некоторых идеологий
DevOps, в частности, All As A Code, что потом очень хорошо ложится
в любую VCS вроде git, поэтому описание проекта, схема и документация
в asciidoc+plantuml.

=== Стоимость

Поскольку проект предназначен для использования в небольших
организациях или в личных целях, либо в каких-то небольших
научных исследованиях с малым бюджетом, я предполагал
применение только свободно распространяемых решений.

Таким образом, стоимость ПО для данного проекта нулевая.

=== Используемое программное обеспечение

* Streamsets (в учебном docker-контейнере)
* Cloudera CDH 5.7 (в учебном docker-контейнере)
* Spark 2.4.4
* Scala 2.11.12
* OS Linux Fedora 31 workstation

==== Вспомогательное ПО

* VS code
* Intelligent IDE Community edition with scala plugin
* Сборщик sbt
* PlantUML: построение UML диаграмм as a code
* Git + github
* Docker
* Oracle Virtualbox

=== Источник данных

Информация с сайта источника http://openweathermap.org
(при помощи rest api)

=== Хранилище

Типа Datalake

=== Витрины и бизнес-кейсы

Отслеживание погодных изменений по Одессе

=== Схема проекта

image::http://www.plantuml.com/plantuml/svg/bLLVQoH547_lJr7a7jJLF5mU0yfT54uXI6315oDBPwQJ7J8xlVRq9iOZi5cDV_0WU0ZwPBm3dvtSPSXSSfbyXUflP7LbHdSccpyPM7Qx_logVbM_tdcKMcdiQ103ld3xcEDRp00lt1XBzmFcRi_jOIP4kXabGsda03xMWw5EL6AxTYTMG9aBGhm3iLgtO3MOQADl8OoC2coa4_9OFVIEDMsj7anvH40WolIL39KH7mA-R5IdmwcROBO8-3jc-DgDtNTuWgUOVG1kdo8oFCUSFWNcW0NmESCpEfyG10KV-OFxaSmbu36Mn6L2vWjlm3UKW-VkUpAFVLKYMs0XL19M3ObORT29s_Va14nDoCTT-eb6h4SXpE4V7XzBBbZWARnhL6hXySfdxys1J873UgaUcK31WsK_fgLufE2fs1KYqLPLezFhZL01zE1ppzNzLBCzPhBdxXVCsU_sm0-6FYM-ndBU4oVwPNiEx2NYpKulwO3lGOZ7WTK6vXeLtwmcU4Z9P-uPdj3tAymeF_TdfgQ-iQFKZsoT1A8e-UzgFGLN_XdprOGI2Xfz1fz8As5PRYfEvOnU6AU-wbF1R6gKvvGx8QJ2CxqItGi7yE29ZaDAyrdJcOUKbUCnqyoetRDBeRW3HjcAr7RgqobnwWbgOODHGsrixo5yAQ3rN0UD-SD6x5StGo_S2thX6kXTSRN0chH1dyZrZ5ed0VPRXJgT9H_GwJIiy_Gq3FWd7h5Ufhe0Kio4D9ExYNVS0d28EQ-knEDQV7p_dYs2Q3BPRMwTAhDwZdovMkSjqLyLGshbHdsjdXYbagdbXgIuDPbwyBzOjGTysKkFHae-0FmLVrjDEfqlbB5H83iTnW_NQjILADcyLYmo2VhKm427Akw5s_rPc_t7tqmthikzvQdm_G6ttxehsRsNpLrUk--kDKeEkigcy3vqXz9iqdSQo5ZU9eALQAYMeqH1Dugtb5cMEyeq0bXAYv1EWxPbTC4woFZwls99UI7DK1UpcFz_osx_Pfsye93n7PHXvFP_kf0xCsNnkUv-7RCA05zMVrEVHbkgbW51t282YkXvl5dBdztelRViIyHQXZSi-CuBNJ8w-LPTNMZxJGjkZvkdPS2jcscyjgZuHwt6EQPfyb7LW1cY-bsRMHALepgng_u35s9NF599E1h4_m80[]

== Реализация

=== Источник данных

* Проведена регистрация на сайте http://openweathermap.org
* Получен ключ для использования API в приложениях

=== Хранилище

В качестве хранилища данных использовал клаудеровские HDFS и Hive.
Остановился на этом, т.к. на большее не позволяло время, ресурсы
компьютера, да и проект декларируется как легкий и бюджетный.

Все это в предустановленном учебном контейнере quickstart.cloudera.

==== Бронзовый слой

Находится в HDFS в виде сырых json-файлов, поступивших от openweathermap.

Размер одного файла около 1МиБ (в учебных целях, в реальности больше).

Эти файлы не удаляются, в чем и есть предназначение бронзового слоя,
в любой момент из них можно повторно загрузить необходимую информацию.

==== Серебряный слой

Организован по принципу Ральфа Кимбалла.

Проект нацелен на бюджетность и его реализация была сильно ограничена во времени.
Поэтому я пришел к выводу, что нужно делать все максимально просто, эффективно и
для "здесь и сейчас".

Метод организации хранилища по принципу "что сейчас нужно для клиента, то и реализуем"
здесь, как мне кажется, подходит.

Для каждого бизнес-кейса в хранилище реализуется своя отдельная таблица и в спарковой,
джобе, которая перекачивает данные из необработанных в хранилище, для каждого
такого кейса предусмотрен отдельный блок кода и отдельный запрос.

===== Примечания

====== Что задумывалось

Задумывалась якорная модель хранилища и ее частичная реализация.

Мне вообще нравится якорная модель. Она мощная и гибкая.
Правда, громоздкая и сложно реализуемая.

Однако, из-за нехватки времени и декларирования проекта как
"быстро, бюджетно, здесь и сейчас" я остановился на принципе Кимбалла.

Кроме того, якорная модель мной уже подробно изучена в одном из ДЗ,
где я применил ее к google bigquery.

====== Локальный запуск спарка

Очень хотел запустить спарковую джобу на quickstart.cloudera на yarn, но
там старый спарк 1.6 и ява 1.7 (на них работает клаудеровский контейнер).

Если на спарк 1.6 я еще перевел, то установить яву 1.7 на свою ОС
мне не удалось по причине того, что нужно много чего даунгрейдить,
она как-то плохо уживается вместе с явой 1.8 и выше.

Потратил много времени, безуспешно пытаясь это все побороть.

В итоге решил запускать спарк локально в самом контейнере стримсета
(туда я пробросил каталог со спарком 2.4.4).

==== Золотой слой или витрины

Витрины у нас фактически лежат в серебряном слое, по методу Кимбалла,
это, конечно, сильно упрощенный вариант.

Поэтому витрины золотого слоя, datamart, которые видит непосредственно клиент,
по задумке этого проекта предполагают быть в виде представлений или просто
запросов к тем витринам, что лежат в хранилище данных.

=== Оркестрация

Оркестрация реализована при помощи Streamsets, который управляет

* Запуском основных потоков (пайплайнов)
* Вызывает раз в две секунды API источника для выкачки информации
* Событиями, которыа генерируются для вызова спарковой джобы (перекачка в сильвер-слой)
* Слушателем, который запускает по событиям выше спарковую джобу

=== Мониторинг

Функции мониторинга также может взять на себя Streamsets.

В процессе работы над проектом я запрограммировал его на запись различных
текстовых журналов, которые можно мониторить при помощи внешней системы типа того же ELK

Однако сам Streamsets очень хорошо и наглядно позволяет мониторить все этапы
по кр. мере бронзового слоя, начиная от обращения к источнику и заканчивая
укладкой данных в бронзовый слой и вызовом спарковой джобы для перекачки в сильвер.

На всех этих этапах видно, когда происходят ошибки и что-то ломается, видна
пропускная способность каждого этапа, количество переданных/принятых данных,
их перцентиль и прочее. +
Все это отлично визуализировано.

== Содержимое проекта

=== Streamsets

==== Пайплайны

image::1-4-2020-16-25-42-PM.png[] 

===== Главный оркестратор

image::1-4-2020-16-30-14-PM.png[] 

===== Загрузчик источника в бронзовый слой

image::1-4-2020-16-32-44-PM.png[] 

===== Оркестратор и прослушиватель для вызова спарковой джобы

image::1-4-2020-16-34-33-PM.png[] 

=== Cloudera

=== Spark

==== Job для загрузки в сильвер-слой.

[source, scala]
----
package org.kliusa.otusde201911.project.jsonprocessor

import java.text.SimpleDateFormat
import java.util.{Calendar, TimeZone}

import org.apache.log4j._
import org.apache.spark.sql.{SaveMode, SparkSession}

/*
* Модуль загружает из бронзового слоя в серебряный
* данные по каждому безнес-кейсу.
* Код, относящийся к каждому кейсу, помечен соотв. комментарием
*
* // CASE OdessaWatch - Кейс для отслеживания параметров погоды в Одессе.
* 
*/

object JsonProcessor extends App{

  val jsonName = args(0)
  val outPath = args(1)

  val currDateStr = new SimpleDateFormat("yyyy-MM-dd-HH-mm-ss").format(
    Calendar.getInstance( TimeZone.getTimeZone("Europe/Kiev") ).getTime()
  )

  // CASE OdessaWatch
  // Запрос для бизнес-кейса
  val sqlOdessaWatch =
    "select" +
    "    '"+currDateStr+"' as date_string, " +
    "    name," +
    "    main_feels_like," +
    "    main_pressure, main_humidity," +
    "    wind_speed, wind_deg," +
    "    rain, snow, clouds_all" +
    "  from json" +
    "  where id = 698740" +
    "  group by" +
    "    name," +
    "    main_feels_like," +
    "    main_pressure, main_humidity," +
    "    wind_speed, wind_deg," +
    "    rain, snow, clouds_all"

  BasicConfigurator.configure()

  Logger.getRootLogger
    .setLevel( Level.ERROR )

  val sparkSession = SparkSession.builder()
    .config("hive.metastore.uris", "thrift://quickstart.cloudera:9083")
    .enableHiveSupport()
    .getOrCreate()

  val jsonDf = sparkSession.read.format("json")
    .load(jsonName) // Датафрейм, в который загружаем исходный джейсон

  jsonDf.createOrReplaceTempView("json")  // Представление по датафрейму исходного джейсона для запросов

  // CASE OdessaWatch
  val dfOdessaWatch = sparkSession.sql(
    sqlOdessaWatch // Запрос по исходному джейсону для формирования целевого датасета
  ) // Целевой датафрейм для загрузки в БД

  // CASE OdessaWatch
  // Пишем целевой датасет в промежуточный авро (может использоваться в стримсетс)
  dfOdessaWatch.coalesce(1)
    .write.format("avro").mode(SaveMode.Append).save(outPath+"/OdessaWatch/"+currDateStr)

  // CASE OdessaWatch
  // Создаем представление для целевого датасета.
  // Из него потом запросом выгрузим в БД
  dfOdessaWatch.createOrReplaceTempView("viewOdessaWatch")  // на основе выборки

  // CASE OdessaWatch
  // Создаем таблицу в хайве, если ее вдруг нет
  val hiveCreateSql = sparkSession.sql("create table if not exists odessawatch as select * from viewOdessaWatch where 0=1")

  // CASE OdessaWatch
  // Заливаем туда данные
  val hiveInsertSql = sparkSession.sql("insert into odessawatch select * from viewOdessaWatch")

}
----

== Примечания

=== Установка и разовая настройка

==== Клонируем репозиторий

Чтобы не грузить репозиторий гита, я смапил стейджы (библиотеки) streamsets в каталог `~/sdc-stagelibs`
(хотя, можно было включить в .gitignore, но уже поздно, сделал так, а проект сдавать :))

==== Каталоги

Необходим каталог `~/sdc-stagelibs`. +
Взять у меня или создать. Если создать, то ниже будет рассказано как его заполнить.

Необходим каталог `~/spark-2.4.4-bin-hadoop2.7` +
Скачать или взять у меня.

==== Права

Перейти в каталог гитового репозитория

[source, bash]
----
chmod 777 -R ~/sdc-stagelibs
chmod 777 -R project/sdc-data
chmod 777 -R project/sdc-stagelibs
chmod 777 -R project/shared_cloudera_quickstart
----

==== Запуск

`docker-compose up`

==== Разовая донастройка

Если до этого содержимое каталога `~/sdc-stagelibs` не было получено, то:

* Открыть в streamsets все пайплайны
* На какие стейджы стримсетс будет ругаться что отсутствует:
**  доустановить (меню пакетов в стримсетс), это разово, они потом сохраняться в
  `~/sdc-stagelibs`

==== Иоформация

Streamsets info

* location locahost: 18630
* credentials
** admin/admin

cloudera-quickstart info

* credentials
** claudera manager: admin/admin
** hue: claudera/claudera
* versions
** CDH 5.7.0
** Hadoop 2.6.0
** Hive 1.1.0

